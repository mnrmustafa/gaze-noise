<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Eye Detection Sound</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <style>
    body { margin: 0; display: flex; flex-direction: column; align-items: center; }
    video, canvas { transform: scaleX(-1); }
  </style>
</head>
<body>
  <h2>Eye Tracking Sound Test</h2>
  <video id="video" autoplay playsinline></video>
  <canvas id="output"></canvas>
  <audio id="noise" src="sound1.mp3"></audio>

  <script>
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');
    const audio = document.getElementById('noise');

    // Eye open/close detection
    let eyesPreviouslyClosed = true; 

    function getEAR(landmarks, left) {
      // Left eye indices (Mediapipe FaceMesh)
      const idx = left ? [33, 159, 133, 145] : [263, 386, 362, 374];
      const [p1, p2, p3, p4] = idx.map(i => landmarks[i]);

      const vertical = Math.hypot(p2.x - p4.x, p2.y - p4.y);
      const horizontal = Math.hypot(p1.x - p3.x, p1.y - p3.y);

      return vertical / horizontal;
    }

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    faceMesh.onResults((results) => {
      canvasElement.width = videoElement.videoWidth;
      canvasElement.height = videoElement.videoHeight;

      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

      if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
        const landmarks = results.multiFaceLandmarks[0];
        const leftEAR = getEAR(landmarks, true);
        const rightEAR = getEAR(landmarks, false);
        const ear = (leftEAR + rightEAR) / 2;

        const threshold = 0.4; // tweak if needed
        const eyesOpen = ear > threshold;

        if (eyesOpen && eyesPreviouslyClosed) {
          audio.currentTime = 0;
          audio.play();
        }

        eyesPreviouslyClosed = !eyesOpen;

        drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, {color: 'lime'});
        drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, {color: 'lime'});
      }
      canvasCtx.restore();
    });

    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({image: videoElement});
      },
      width: 640,
      height: 480
    });
    camera.start();
  </script>
</body>
</html>
